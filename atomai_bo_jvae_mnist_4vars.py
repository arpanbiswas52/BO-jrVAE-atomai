# -*- coding: utf-8 -*-
"""atomAI_BO-jVAE_MNIST_4vars.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pFSSYf24Rqsthbupwoh0otkSAlgEvr9H

This code is run in SDGX box
"""

#!pip install atomai
#!pip install botorch #version 0.5.1
#!pip install gpytorch #version 1.6.0
#!pip install -q pyroved kornia

#@title Imports
import numpy as np

import torch

from torchvision import datasets, transforms

import atomai as aoi
import kornia as K
import kornia.metrics as metrics
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

from PIL import Image


from typing import Tuple

import random

# Import GP and BoTorch functions
import gpytorch as gpt
from botorch.models import SingleTaskGP, ModelListGP
#from botorch.models import gpytorch
from botorch.fit import fit_gpytorch_model
from botorch.models.gpytorch import GPyTorchModel
from botorch.utils import standardize
from gpytorch.distributions import MultivariateNormal
from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel
from gpytorch.likelihoods import GaussianLikelihood
from gpytorch.means import ConstantMean, LinearMean
from gpytorch.mlls import ExactMarginalLogLikelihood
from botorch.acquisition import UpperConfidenceBound
from botorch.optim import optimize_acqf
from botorch.acquisition import qExpectedImprovement
from botorch.acquisition import ExpectedImprovement
from botorch.sampling import IIDNormalSampler
from botorch.sampling import SobolQMCNormalSampler
from gpytorch.likelihoods.likelihood import Likelihood
from gpytorch.constraints import GreaterThan

from botorch.generation import get_best_candidates, gen_candidates_torch
from botorch.optim import gen_batch_initial_conditions

from gpytorch.models import ExactGP
from mpl_toolkits.axes_grid1 import make_axes_locatable
#from smt.sampling_methods import LHS
from torch.optim import SGD
from torch.optim import Adam
from scipy.stats import norm
from scipy.interpolate import interp1d

#@title Helper functions
def rotate_images(imgdata, rotation_range: Tuple[int]) -> Tuple[torch.Tensor]:
    torch.manual_seed(0)
    theta = torch.randint(*rotation_range, size=(len(imgdata),))
    imgdata = K.geometry.rotate(imgdata.float()[:, None], theta.float())
    imgdata = imgdata / imgdata.max()
    return imgdata, theta


def get_mnist_data(rotation_range: Tuple[int]) -> Tuple[torch.Tensor]:
    !wget -qq www.di.ens.fr/~lelarge/MNIST.tar.gz
    !tar -zxf MNIST.tar.gz
    mnist_trainset = datasets.MNIST(
        root='.', train=True, download=False, transform=None)
    images_r, theta = rotate_images(mnist_trainset.data, rotation_range)
    return images_r, mnist_trainset.targets, theta

"""#Functions defined for problem and objectives"""

#@title SSIM loss function between images
def ssim_loss(
    img1: torch.Tensor,
    img2: torch.Tensor,
    window_size: int,
    max_val: float = 1.0,
    eps: float = 1e-12,
    reduction: str = 'mean',
) -> torch.Tensor:
    r"""Function that computes a loss based on the SSIM measurement.

    The loss, or the Structural dissimilarity (DSSIM) is described as:

    .. math::

      \text{loss}(x, y) = \frac{1 - \text{SSIM}(x, y)}{2}

    See :meth:`~kornia.losses.ssim` for details about SSIM.

    Args:
        img1: the first input image with shape :math:`(B, C, H, W)`.
        img2: the second input image with shape :math:`(B, C, H, W)`.
        window_size: the size of the gaussian kernel to smooth the images.
        max_val: the dynamic range of the images.
        eps: Small value for numerically stability when dividing.
        reduction : Specifies the reduction to apply to the
         output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
         ``'mean'``: the sum of the output will be divided by the number of elements
         in the output, ``'sum'``: the output will be summed.

    Returns:
        The loss based on the ssim index.

    Examples:
        >>> input1 = torch.rand(1, 4, 5, 5)
        >>> input2 = torch.rand(1, 4, 5, 5)
        >>> loss = ssim_loss(input1, input2, 5)
    """
    # compute the ssim map
    ssim_map: torch.Tensor = metrics.ssim(img1, img2, window_size, max_val, eps)

    # compute and reduce the loss
    loss = torch.clamp((1.0 - ssim_map) / 2, min=0, max=1)

    if reduction == "mean":
        loss = torch.mean(loss)
    elif reduction == "sum":
        loss = torch.sum(loss)
    elif reduction == "none":
        pass
    return loss

#@title SSIM Loss objective function- Combined objective to minimize the ssim among the manifolds representing discrete classes, thus maximize the loss; and to maximize the ssim within the manifolds representing each discrete classes, thus minimize the loss
def loss_obj(X, data, hyp_iter, batch_size, training_cycles, B, H, W, discrete_dim):
    # xx=float(X)
    pen = 10**0
    input_dim = (H, W)
    M = np.zeros((H*B, W*B, discrete_dim))
    loss1 = 0
    loss2 = 0
    cc = X[0, 0]
    cz = X[0, 1]
    gamma_cc = X[0, 2]
    gamma_cz = X[0, 3]
    jvae_X = aoi.models.jrVAE(input_dim, latent_dim=2, discrete_dim=[discrete_dim],
                        numlayers_encoder=3, numhidden_encoder=512,
                        numlayers_decoder=3, numhidden_decoder=512,
                        translation=True, seed=42, skip=False)

    jvae_X.fit(data, training_cycles=training_cycles, batch_size=batch_size, loss="ce",
          cont_capacity=[cc, hyp_iter, gamma_cc], disc_capacity=[cz, hyp_iter, gamma_cz], verbose ="False")


    for i in range(discrete_dim):
        #plt.figure()
        M[:,:,i] = jvae_X.manifold2d(d=B, disc_idx=i, savefig = '1')
        #plt.close()

    M = torch.from_numpy(M)
    M = torch.reshape(M, (H, B, W, B, M.shape[2]))
    M = M.permute(1, 3, 0, 2, 4)
    M = torch.reshape(M, (M.shape[0]*M.shape[1], M.shape[2], M.shape[3], M.shape[4]))
    M = torch.reshape(M, (M.shape[0], 1, M.shape[1], M.shape[2], M.shape[3]))
    #M = torch.reshape(M, (1, 1, M.shape[0], M.shape[1], M.shape[2]))
    k1 = 0
    #Objective 1 is to minimize the ssim among the manifolds representing discrete classes, thus maximize the loss
    for i in range(discrete_dim):
        for j in range(discrete_dim):
            if (j > i):
                M1 = M[:,:,:,:,i]
                M2 = M[:,:,:,:,j]
                # Compute SSIM/loss among each manifolds
                loss1 = loss1 + ssim_loss(M1, M2, 5)
                k1 = k1 + 1
    #obj1 = (loss1/k1)*pen
    obj1 = (loss1)*pen

    #Objective 2 is to maximize the ssim within the manifolds representing each discrete classes, thus minimize the loss
    
    np.random.seed(0)
    n_image = 1000
    idxy = np.random.randint(0, B*B, (n_image, 2))
    for i in range(discrete_dim):
      k2 = 0
      l2 = 0
      for j in range(n_image):
        if (idxy[i, 0] is not idxy[i, 1]):
          loc1 = idxy[i, 0]
          loc2 = idxy[i, 1]
          m1 = M[loc1, :, :, :, i]
          m2 = M[loc2, :, :, :, i]
          m1 = torch.reshape(m1, (1, m1.shape[0], m1.shape[1], m1.shape[2]))
          m2 = torch.reshape(m2, (1, m2.shape[0], m2.shape[1], m2.shape[2]))
          # Compute SSIM/loss within each manifolds
          l2 = l2 + ssim_loss(m1, m2, 5)
          k2 = k2 + 1
      
      loss2 = loss2 + (l2/k2)

    #obj2 = (loss2/discrete_dim)*pen
    obj2 = (loss2)*pen
    
    obj = obj1 - obj2 # obj2 converted into maximization problem
    obj = obj1
  
    return obj

"""
Below section defines the list of functions (user calls these functions during analysis):
1. Gaussian Process
2. Optimizize Hyperparameter of Gaussian Process (using Adam optimizer)
3. Posterior means and variance computation
4. Acquistion functions for BO- Expected Improvmement"""

class SimpleCustomGP(ExactGP, GPyTorchModel):
    _num_outputs = 1  # to inform GPyTorchModel API

    def __init__(self, train_X, train_Y):
        # squeeze output dim before passing train_Y to ExactGP
        super().__init__(train_X, train_Y.squeeze(-1), GaussianLikelihood())
        self.mean_module = ConstantMean()
        #self.mean_module = LinearMean(train_X.shape[-1])
        self.covar_module = ScaleKernel(
            #base_kernel=MaternKernel(nu=2.5, ard_num_dims=train_X.shape[-1]),
            base_kernel=RBFKernel(ard_num_dims=train_X.shape[-1]),
        )
        self.to(train_X)  # make sure we're on the right device/dtype

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return MultivariateNormal(mean_x, covar_x)

#Optimize Hyperparameters of GP#
def optimize_hyperparam_trainGP(train_X, train_Y):
    # Gp model fit

    gp_surro = SimpleCustomGP(train_X, train_Y)
    gp_surro = gp_surro.double()
    gp_surro.likelihood.noise_covar.register_constraint("raw_noise", GreaterThan(1e-1))
    mll1 = ExactMarginalLogLikelihood(gp_surro.likelihood, gp_surro)
    # fit_gpytorch_model(mll)
    mll1 = mll1.to(train_X)
    gp_surro.train()
    gp_surro.likelihood.train()
    ## Here we use Adam optimizer with learning rate =0.1, user can change here with different algorithm and/or learning rate for each GP
    optimizer1 = Adam([{'params': gp_surro.parameters()}], lr=0.1)
    #optimizer1 = SGD([{'params': gp_surro.parameters()}], lr=0.0001)

    NUM_EPOCHS = 150

    for epoch in range(NUM_EPOCHS):
        # clear gradients
        optimizer1.zero_grad()
        # forward pass through the model to obtain the output MultivariateNormal
        output1 = gp_surro(train_X)
        # Compute negative marginal log likelihood
        loss1 = - mll1(output1, gp_surro.train_targets)
        # back prop gradients
        loss1.backward(retain_graph=True)
        # print last iterations
        if (epoch + 1) > NUM_EPOCHS: #Stopping the print for now
            print("GP Model trained:")
            print("Iteration:" + str(epoch + 1))
            print("Loss:" + str(loss1.item()))
            # print("Length Scale:" +str(gp_PZO.covar_module.base_kernel.lengthscale.item()))
            print("noise:" + str(gp_surro.likelihood.noise.item()))


        optimizer1.step()

    gp_surro.eval()
    gp_surro.likelihood.eval()
    return gp_surro


#GP posterior predictions#
def cal_posterior(gp_surro, test_X):
    y_pred_means = torch.empty(len(test_X), 1)
    y_pred_vars = torch.empty(len(test_X), 1)
    t_X = torch.empty(1, test_X.shape[1])
    for t in range(0, len(test_X)):
        with torch.no_grad(), gpt.settings.max_lanczos_quadrature_iterations(32), \
            gpt.settings.fast_computations(covar_root_decomposition=False, log_prob=False,
                                                      solves=True), \
            gpt.settings.max_cg_iterations(100), \
            gpt.settings.max_preconditioner_size(80), \
            gpt.settings.num_trace_samples(128):

                t_X[:, 0] = test_X[t, 0]
                t_X[:, 1] = test_X[t, 1]
                t_X[:, 2] = test_X[t, 2]
                t_X[:, 3] = test_X[t, 3]
                #t_X = test_X.double()
                y_pred_surro = gp_surro.posterior(t_X)
                y_pred_means[t, 0] = y_pred_surro.mean
                y_pred_vars[t, 0] = y_pred_surro.variance

    return y_pred_means, y_pred_vars


#EI acquistion function#
def acqmanEI(y_pred_means, y_pred_vars, train_Y):


    y_pred_means = y_pred_means.detach().numpy()
    y_pred_vars = y_pred_vars.detach().numpy()
    fmax = train_Y.max()
    fmax = fmax.detach().numpy()
    best_value = fmax
    EI_val = np.zeros(len(y_pred_vars))
    Z = np.zeros(len(y_pred_vars))
    eta = 0.001
    
    for i in range(0, len(y_pred_vars)):
        if (y_pred_vars[i] <=0):
            EI_val[i] = 0
        else:
            Z[i] =  (y_pred_means[i]-best_value-eta)/y_pred_vars[i]
            EI_val[i] = (y_pred_means[i]-best_value-eta)*norm.cdf(Z[i]) + y_pred_vars[i]*norm.pdf(Z[i])

    acq_val = np.max(EI_val)
    acq_cand = [k for k, j in enumerate(EI_val) if j == acq_val]
    return acq_cand, acq_val, EI_val

"""#Other list of functions-

1. Evaluate initial data and normalize all data
2. Evaluate functions for new data and augment data
"""

# Normalize all data. It is very important to fit GP model with normalized data to avoid issues such as
# - decrease of GP performance due to largely spaced real-valued data X.
def normalize_get_initialdata_KL(X, fix_params, data, num_rows, num, m):
    
    X_feas = torch.empty((X.shape[1]**X.shape[0], X.shape[0]))
    k=0
    for t1 in range(0, X.shape[1]):
        for t2 in range(0, X.shape[1]):
          for t3 in range(0, X.shape[1]):
            for t4 in range(0, X.shape[1]):
                X_feas[k, 0] = X[0, t1]
                X_feas[k, 1] = X[1, t2]
                X_feas[k, 2] = X[2, t3]
                X_feas[k, 3] = X[3, t4]
                k=k+1
    
    X_feas_norm = torch.empty((X_feas.shape[0], X_feas.shape[1]))
    #train_X = torch.empty((len(X), num))
    #train_X_norm = torch.empty((len(X), num))
    train_Y = torch.empty((num, 1))

    # Normalize X
    for i in range(0, X_feas.shape[1]):
        X_feas_norm[:, i] = (X_feas[:, i] - torch.min(X_feas[:, i])) / (torch.max(X_feas[:, i]) - torch.min(X_feas[:, i]))
      
    

    # Select starting samples randomly as training data
    np.random.seed(0)
    idx = np.random.randint(0, len(X_feas), num)
    train_X = X_feas[idx]
    train_X_norm = X_feas_norm[idx]
    #print(train_X, train_X_norm)

    #Evaluate initial training data
    x = torch.empty((1,train_X.shape[1]))
    for i in range(0, num):
        x[0, 0] = train_X[i, 0]
        x[0, 1] = train_X[i, 1]
        x[0, 2] = train_X[i, 2]
        x[0, 3] = train_X[i, 3]

        print("Function eval #" + str(m + 1))
        hyp_iter, batch_size, training_cycles, B, H, W, discrete_dim  = fix_params[0], fix_params[1], fix_params[2], fix_params[3], fix_params[4], fix_params[5], fix_params[6]

        train_Y[i, 0] = loss_obj(x, data, hyp_iter, batch_size, training_cycles, B, H, W, discrete_dim)
        m = m + 1

    return X_feas, X_feas_norm, train_X, train_X_norm, train_Y, m


################################Augment data - Existing training data with new evaluated data################################
def augment_newdata_KL(acq_X, acq_X_norm, train_X, train_X_norm, train_Y, fix_params, data, m):
    nextX = acq_X
    nextX_norm = acq_X_norm
    #train_X_norm = torch.cat((train_X_norm, nextX_norm), 0)
    #train_X_norm = train_X_norm.double()
    train_X_norm = torch.vstack((train_X_norm, nextX_norm))
    train_X = torch.vstack((train_X, nextX))
    next_feval = torch.empty(1, 1)
    x = torch.empty((1,train_X.shape[1]))
    x[0, 0] = train_X[-1, 0]
    x[0, 1] = train_X[-1, 1]
    x[0, 2] = train_X[-1, 2]
    x[0, 3] = train_X[-1, 3]

    print("Function eval #" + str(m + 1))
    hyp_iter, batch_size, training_cycles, B, H, W, discrete_dim  =\
    fix_params[0], fix_params[1], fix_params[2], fix_params[3], fix_params[4], fix_params[5], fix_params[6]

    next_feval[0, 0] = loss_obj(x, data, hyp_iter, batch_size, training_cycles, B, H, W, discrete_dim)

    train_Y = torch.vstack((train_Y, next_feval))

    #train_Y = torch.cat((train_Y, next_feval), 0)
    m = m + 1
    return train_X, train_X_norm, train_Y, m

#@title Functions to plot KL trajectories at specific BO iterations (Need to revise)
def plot_iteration_results(train_X, train_Y, test_X, y_pred_means, y_pred_vars, dim):
    pen = 10**0
    #Best solution among the evaluated data
    
    loss = torch.max(train_Y)
    ind = torch.argmax(train_Y)
    X_opt = torch.empty((1,train_X.shape[1]))
    #X_opt = train_X[ind, :]
    X_opt[0, 0] = train_X[ind, 0]
    X_opt[0, 1] = train_X[ind, 1]
    X_opt[0, 2] = train_X[ind, 2]
    X_opt[0, 3] = train_X[ind, 3]


    # Best estimated solution from GP model considering the non-evaluated solution

    loss = torch.max(y_pred_means)
    ind = torch.argmax(y_pred_means)
    X_opt_GP = torch.empty((1,test_X.shape[1]))
    #X_opt = train_X[ind, :]
    X_opt_GP[0, 0] = test_X[ind, 0]
    X_opt_GP[0, 1] = test_X[ind, 1]
    X_opt_GP[0, 2] = test_X[ind, 2]
    X_opt_GP[0, 3] = test_X[ind, 3]

    if (dim == 2):
        #Objective map over 2D latent space
        plt.figure()
        a = plt.scatter(test_X[:,0], test_X[:,1], c=y_pred_means/pen, cmap='viridis', linewidth=0.2)
        plt.scatter(train_X[:,0], train_X[:,1], marker='o', c='g')
        plt.scatter(X_opt[0, 0], X_opt[0, 1], marker='x', c='r')
        plt.scatter(X_opt_GP[0, 0], X_opt_GP[0, 1], marker='o', c='r')
        plt.xlabel('C_c')
        plt.ylabel('C_z')
        plt.title('Objective (SSIM loss) map over feasible 2D latent space')
        plt.colorbar(a)
        plt.show()
        
        #Objective map over 2D latent space
        plt.figure()
        plt.scatter(test_X[:,0], test_X[:,1], c=y_pred_vars/(pen**2), cmap='viridis', linewidth=0.2)
        plt.xlabel('C_c')
        plt.ylabel('C_z')
        plt.title('Objective var map over feasible 2D latent space')
        plt.colorbar()
        plt.show()

    return X_opt, X_opt_GP

#@title BO framework- Integrating the above functions
def BO_jrVAE(X, fix_params, data, num_rows, num_start, N, dim=2):
    num = num_start
    m = 0
    # Initialization: evaluate few initial data normalize data
    test_X, test_X_norm, train_X, train_X_norm, train_Y, m = \
        normalize_get_initialdata_KL(X, fix_params, data, num_rows, num, m)


    print("Initial evaluation complete. Start BO")
    ## Gp model fit
    # Calling function to fit and optimizize Hyperparameter of Gaussian Process (using Adam optimizer)
    # Input args- Torch arrays of normalized training data, parameter X and objective eval Y
    # Output args- Gaussian process model lists
    gp_surro = optimize_hyperparam_trainGP(train_X_norm, train_Y)

    for i in range(1, N + 1):
        # Calculate posterior for analysis for intermidiate iterations
        y_pred_means, y_pred_vars = cal_posterior(gp_surro, test_X_norm)
        if (i == 1 or i == 10 or i == 30 or i == 50 or i == 80 or i == 100 or i == 125 or i == 150 or i == 200):
            # Plotting functions to check the current state exploration and Pareto fronts
            X_eval, X_GP = plot_iteration_results(train_X, train_Y, test_X, y_pred_means, y_pred_vars, dim)
            print("Current best solutions")
            print("Evaluated",X_eval)
            print("Estimated", X_GP)
        acq_cand, acq_val, EI_val = acqmanEI(y_pred_means, y_pred_vars, train_Y)
        val = acq_val
        ind = np.random.choice(acq_cand)

        ################################################################
        ## Find next point which maximizes the learning through exploration-exploitation
        if (i == 1):
            val_ini = val
        # Check for convergence
        if ((val) <= 0):  # Stop for negligible expected improvement
            print("Model converged due to sufficient learning over search space ")
            break
        else:
            nextX = torch.empty((1, len(X)))
            nextX_norm = torch.empty(1, len(X))
            nextX[0,:] = test_X[ind, :]
            nextX_norm [0, :] = test_X_norm[ind, :]

            # Evaluate true function for new data, augment data
            train_X, train_X_norm, train_Y, m = augment_newdata_KL(nextX, nextX_norm, train_X, train_X_norm,train_Y, fix_params, data, m)

            # Gp model fit
            # Updating GP with augmented training data
            gp_surro = optimize_hyperparam_trainGP(train_X_norm, train_Y)

    ## Final posterior prediction after all the sampling done

    if (i == N):
        print("Max. sampling reached, model stopped")

    #Optimal GP learning
    gp_opt = gp_surro
    # Posterior calculation with converged GP model
    y_pred_means, y_pred_vars = cal_posterior(gp_opt, test_X_norm)
    # Plotting functions to check final iteration
    X_opt, X_opt_GP = plot_iteration_results(train_X, train_Y, test_X, y_pred_means, y_pred_vars, dim)

    return  X_opt, X_opt_GP, gp_opt, train_X, train_Y, test_X, y_pred_means, y_pred_vars

"""#Now we start Analysis 1

MNIST dataset with 10 digits
"""

#!pip install -U gdown
#!gdown "https://drive.google.com/uc?id=14lYofhGcKCOLU4Js2vnQ9m11TaHBTNPR"

train_data_r = np.load("train_data_r.npy")

print(train_data_r.shape)

"""Subsetting Data"""

np.random.seed(10)
n_data = 1000
idx = np.random.randint(0, len(train_data_r), n_data)
train_data_ss = train_data_r[idx]
print(train_data_ss.shape)

"""Run BO

We want to optimize C_cont, C_discrete and gamma as these are the parameter of interests as per suggestion https://arxiv.org/pdf/1804.03599.pdf 

cont_capacity (list) – List containing (C_cont, num_iters, gamma) parameters to control the capacity of the continuous latent channel. Default values: [C_cont, iter, gamma]. 

disc_capacity (list) – List containing (C_discrete, num_iters, gamma) parameters to control the capacity of the discrete latent channel(s). Default values: [C_discrete, iter, gamma].

Case Study 1- We start with optimizing the maximum value C_cont and C_discrete variables with a large search space.

-- We fixed the gamma value (will consider later for optimization as this is in general robust to the model as per https://arxiv.org/pdf/1804.03599.pdf) and iter.

#Running BO
"""

batch_size=64
training_cycles = 300
B = 12 #grid size for manifold2D
#Data dim size
H = 28
W = 28
#Initialize # of discrete class
discrete_dim = 10
hyp_iter = 25000

#Initialize for BO
num_rows =15
num_start = 30  # Starting samples
N= 200

#latent parameters for defining KL trajectories
cc = torch.linspace(1, 15, num_rows)
cz = torch.linspace(1, 15, num_rows)
gcc = torch.linspace(10, 300, num_rows)
gcz = torch.linspace(10, 300, num_rows)

X= torch.vstack((cc, cz, gcc, gcz))


#Fixed parameters of VAE model
fix_params = [hyp_iter, batch_size, training_cycles, B, H, W, discrete_dim]

#BO
X_opt, X_opt_GP, gp_opt, train_X, train_Y, test_X, y_pred_means, y_pred_vars = BO_jrVAE(X, fix_params, train_data_ss, num_rows, num_start, N, dim =4)


print(X_opt, X_opt_GP)
np.save("X_opt", X_opt)
np.save("X_opt_GP", X_opt_GP)
np.save("train_X", train_X)
np.save("train_Y", train_Y)

print(X_opt, X_opt_GP)

"""#Considering Full data for final training of jrVAE with optimized params

GP estimated Optimal solution of C_cont and C_discrete variables, and their respective gamma values
"""

print(X_opt_GP)

batch_size=64
training_cycles = 100
B = 12 #grid size for manifold2D
#Data dim size
H = 28
W = 28
#Initialize # of discrete class
discrete_dim = 10
hyp_iter = 25000
input_dim = (H, W)
C_c = X_opt_GP[0, 0]
C_z = X_opt_GP[0, 1]
gamma_c = X_opt_GP[0, 2]
gamma_z = X_opt_GP[0, 3]

# Intitialize model
jrvae = aoi.models.jrVAE(input_dim, latent_dim=2, discrete_dim=[discrete_dim],
                        numlayers_encoder=3, numhidden_encoder=512,
                        numlayers_decoder=3, numhidden_decoder=512,
                        translation=True, seed =42, skip=False) 
# Train
jrvae.fit(train_data_r, training_cycles=training_cycles, batch_size=batch_size, lr=1e-3, loss="ce",
          cont_capacity=[C_c, hyp_iter, gamma_c], disc_capacity=[C_z, hyp_iter, gamma_z])

#Plot the trained manifolds
for i in range(2):
    plt.figure()
    jrvae.manifold_traversal(i, d= B, cmap='viridis')
    plt.savefig('GP_FinalManifold' +str(i) +'.png')
    plt.show()

for i in range(discrete_dim):
    plt.figure()
    jrvae.manifold2d(d=12, disc_idx=i)
    plt.savefig('GP_Trained_Class' +str(i+1) +'.png')
    plt.show()

"""Evaluated Optimal solution of C_cont and C_discrete variables, and their respective gamma values"""

print(X_opt)

batch_size=64
training_cycles = 100
B = 12 #grid size for manifold2D
#Data dim size
H = 28
W = 28
#Initialize # of discrete class
discrete_dim = 10
hyp_iter = 25000
input_dim = (H, W)
C_c = X_opt[0, 0]
C_z = X_opt[0, 1]
gamma_c = X_opt[0, 2]
gamma_z = X_opt[0, 3]

# Intitialize model
jrvae = aoi.models.jrVAE(input_dim, latent_dim=2, discrete_dim=[discrete_dim],
                        numlayers_encoder=3, numhidden_encoder=512,
                        numlayers_decoder=3, numhidden_decoder=512,
                        translation=True, seed =42, skip=False) 
# Train
jrvae.fit(train_data_r, training_cycles=training_cycles, batch_size=batch_size, lr=1e-3, loss="ce",
          cont_capacity=[C_c, hyp_iter, gamma_c], disc_capacity=[C_z, hyp_iter, gamma_z])

#Plot the trained manifolds
for i in range(2):
    plt.figure()
    jrvae.manifold_traversal(i, d= B, cmap='viridis')
    plt.savefig('Eval_FinalManifold' +str(i) +'.png')
    plt.show()

for i in range(discrete_dim):
    plt.figure()
    jrvae.manifold2d(d=12, disc_idx=i)
    plt.savefig('Eval_Trained_Class' +str(i+1) +'.png')
    plt.show()